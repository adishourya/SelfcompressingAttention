import torch
import sympy as sp
from IPython.display import display
import matplotlib.pyplot as plt
import matplotx
plt.style.use(matplotx.styles.pacoty)


from quant_func import STERound
torch.steRound = STERound.apply


# 12 feature input . 4 class labelling
class Dataset():
    def __init__(self, num_examples=100):
        """
        4-class Classification dataset
        W : R12 -> R4
        y = argmax(X@W + b)
        """

        torch.manual_seed(10)
        
        # Mostly sparse W with small scale and precision to represent them
        # this could be inferenced with fp4 if our model achieves this.
        self._W = torch.tensor([
            [-0.15, 0.0,   0.0,   0.0],
            [ 0.0, 0.15,  0.0,   0.0],
            [ 0.05, 0.0,  0.0,   0.0],
            [ 0.0, -0.1,  0.0,   0.0],
            [ 0.0, 0.0,   0.0,   0.22],
            [ 0.0, 0.0,   0.0,   0.0],
            [ 0.0, 0.0,   0.0,  0.21],
            [ 0.0, 0.0,   0.05,  0.0],
            [ 0.0, 0.0,   0.0,   0.0],
            [ 0.01, 0.0,  0.0,   0.0],
            [ 0.0, 0.0,   0.0,   0.0],
            [ 0.0, 0.0,   0.18,  0.0]
        ], dtype=torch.float32)

        self._bias = 0.1 * torch.tensor([0.2, -0.1, 0.3, -0.05], dtype=torch.float32)

        self.x = torch.randn(num_examples, 12, dtype=torch.float32)
        self.y = self.x @ self._W + self._bias
        self.y = torch.argmax(self.y, dim=-1)

    def show_answer(self, show=4):
        from IPython.display import display

        sp_W = sp.Matrix(self._W)
        sp_x = [sp.Symbol(f"x{i}") for i in range(self._W.shape[0])]
        sp_y = sp.Symbol("y")
        sp_func = sp.Function("argmax")
        sp_b = sp.Matrix(self._bias)

        sp_x = sp.Matrix(sp_x).T
        sp_b = sp.Matrix(sp_b).T

        sp_eq = sp.Eq(sp_y, sp_func(sp_x @ sp_W + sp_b), evaluate=False)

        print("True Pop Params")
        display(sp.Eq(sp.Symbol("W"), sp_W, evaluate=False))
        display(sp.Eq(sp.Symbol("b"), sp_b, evaluate=False))
        display(sp_eq)

# Example usage
data = Dataset()
data.show_answer()



plt.hist(data.y)


#simple NN 1 layer same shape
# register gelu

torch.gelu = torch.nn.GELU()
x = torch.linspace(-10,10,100)
y = torch.gelu(x)

plt.plot(x,y)


class SuperSimpleNN(torch.nn.Module):    
    def __init__(self):
        """
        we will diverge from the paper for now...
        we will assign an exponent and a bit depth for each of the weight and bias matrices
        usually they would be associated with 1 pair and if b reaches 0 we take out..
        that matrix from the graph.. (also optimizer states)
        """
        super().__init__()
        self.W_exp = torch.ones(12,4,dtype=torch.float32)* -8
        self.b_exp = torch.ones(1,4,dtype=torch.float32)* -8 

        # we will start with 4 bit depth for now
        self.W_depth = torch.ones_like(self.W_exp) * 4
        self.b_depth = torch.ones_like(self.b_exp) * 4
        
        self.W = torch.randn(12,4,dtype=torch.float32)
        self.b = torch.randn(1,4,dtype=torch.float32)
        
        self.W = self._quantize(self.W,self.W_exp,self.W_depth)
        self.b = self._quantize(self.b,self.b_exp,self.b_depth)

        # trainables
        self.W = torch.nn.Parameter(self.W)
        self.b = torch.nn.Parameter(self.b)
        self.W_exp = torch.nn.Parameter(self.W_exp)
        self.b_exp = torch.nn.Parameter(self.b_exp)
        self.W_depth = torch.nn.Parameter(self.W_depth)
        self.b_depth = torch.nn.Parameter(self.b_depth)


    @staticmethod
    def _quantize(x,e,b):
        b = torch.relu(b)
        upscale = x/torch.exp2(e)
        half = torch.exp2(b-1)
        clipped = torch.clip(upscale,-1*half,half-1)
        rounded = torch.steRound(clipped)
        return torch.exp2(e) * rounded

    def forward(self,x):
        # quantize every forward pass.. make depth >=0
        W_depth = torch.relu(self.W_depth)
        b_depth = torch.relu(self.b_depth)
        
        W_q = self._quantize(self.W,self.W_exp,W_depth)
        b_q = self._quantize(self.b,self.b_exp,b_depth)
    
        out = torch.gelu(x@W_q + b_q)
        return out

simple_model = SuperSimpleNN()


print("At init")
with torch.no_grad():
    for p in simple_model.named_parameters():
        # print(p)
        display(sp.Eq(sp.Symbol(f"{p[0]}_init"), sp.Matrix(p[1]),evaluate=False))


x_examples = data.x.to("cuda")
y_examples = data.y.to("cuda")
torch.set_float32_matmul_precision("high")

@torch.compile
def train(epochs:int = 100_000):
    model = SuperSimpleNN()
    model.to("cuda")

    from tqdm import tqdm
    pbar = tqdm(range(epochs))
    optim = torch.optim.AdamW(model.parameters(),weight_decay=1e-3)

    for epoch in pbar:
        out = model(x_examples)
        bit_decay = torch.norm(torch.cat([
            p.flatten() for name, p in model.named_parameters()
            if "depth" in name or "exp" in name
            ]))

        loss = torch.nn.functional.cross_entropy(input = out, target = y_examples) + 1e-2 * (bit_decay)
    
        if epoch % 10_000 == 0:
            pbar.set_postfix(loss=loss.item())
    
        optim.zero_grad()
        loss.backward()
        optim.step()
    return model


model = train()


with torch.no_grad():
    # quantize one last time for inference
    Wdepth = torch.relu(model.W_depth)
    bdepth = torch.relu(model.b_depth)

    Wexp = model.W_exp
    bexp = model.b_exp

    W = model.W
    b = model.b
    W = model._quantize(W,Wexp,Wdepth)
    b = model._quantize(b,bexp,bdepth)

    response = torch.argmax((x_examples@W+b),dim=-1)
    print("Score:",torch.sum(response==y_examples))
    

    sp_Wresult = sp.Matrix(W.to("cpu"))
    sp_Bresult = sp.Matrix(b.to("cpu"))
    
    display(sp.Eq(sp.Symbol("W_result"), sp_Wresult,evaluate=False))
    display(sp.Eq(sp.Symbol("B_result"), sp_Bresult,evaluate=False))

    print("==========================")


    sp_Wexpresult = sp.Matrix(model.W_exp.to("cpu"))
    sp_Wdepthresult = sp.Matrix(model.W_depth.to("cpu"))

    display(sp.Eq(sp.Symbol("W_depth_result"), sp_Wdepthresult,evaluate=False))
    display(sp.Eq(sp.Symbol("W_exp_result"), sp_Wexpresult,evaluate=False))
    

    sp_bexpresult = sp.Matrix(model.b_exp.to("cpu"))
    sp_bdepthresult = sp.Matrix(model.b_depth.to("cpu"))

    display(sp.Eq(sp.Symbol("b_depth_result"), sp_bdepthresult,evaluate=False))
    display(sp.Eq(sp.Symbol("b_exp_result"), sp_bexpresult,evaluate=False))
        


# anytime we have exp too low then we can probably get a weight 0
# but it is guarenteed to have 0 weight if depth <=0 . 
