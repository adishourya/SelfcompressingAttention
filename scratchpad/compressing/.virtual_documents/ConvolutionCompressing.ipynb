import torch
import matplotlib.pyplot as plt
import torchvision.datasets as datasets
from torchvision import transforms
from torch.utils.data import DataLoader, random_split
import einops

from quant_func import STERound
from tqdm import tqdm
import sympy as sp

#aliases
torch.steRound = STERound.apply
torch.gelu = torch.nn.functional.gelu
torch.mul_reduce = torch.prod

torch.manual_seed(10)

# tracking stuff
import os
os.getenv("comet_api")

from comet_ml import start
from comet_ml.integration.pytorch import log_model,watch

experiment = start(
  api_key=os.getenv("comet_api"),
  project_name="convolution_compressing",
  workspace="adishourya"
)
experiment.add_tag("low_bval")


# we will use the simplest dataset... ever
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

mnist = datasets.MNIST(root='./data', train=True, download=True,transform=transform)

train_size = int(0.9 * len(mnist))  # 54000
test_size = len(mnist) - train_size  # 6000
train_dataset, test_dataset = random_split(mnist, [train_size, test_size])

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)


for img,labels in train_loader:
    print(img.shape,labels.shape)
    break


len(train_loader) # total training batches


eg,eg_label = img[:5] , labels[:5]
eg = einops.rearrange(eg,"b c h w -> h (b w) c")
plt.imshow(eg,cmap="Blues")
print(eg_label)


class Qconv(torch.nn.Module):
    def  __init__ (self,in_channels,out_channels,kernel_size=3,b=2.0,e=-8.0,):
        """
        # weight tying exp_bits and depth_bits
        # note number  of output  channels  is number of filterkernels  launched
        # we will try to not just compress  but take out entire filter kernels...
        # these asserts them to be pytorch tensors
        """
        super().__init__()
       
        in_channels = torch.as_tensor(in_channels)
        out_channels = torch.as_tensor(out_channels)
        kernel_size = torch.as_tensor(kernel_size)
        b = torch.as_tensor(b)
        e= torch.as_tensor(e)

        # fan_in is just in_channels
        weight_scale = 1/ torch.sqrt(in_channels*out_channels*out_channels)
        self.weight = torch.ones(out_channels,in_channels,kernel_size,kernel_size)
        self.weight = self.weight.uniform_(-weight_scale,weight_scale)

        # 1 for each kernel (out_channel).. to perform safe broadcasting we fill the rest of them with 1
        self.exp_bit = torch.ones(out_channels,1,1,1)*e
        self.depth_bit = torch.ones(out_channels,1,1,1)*b

        # exp and depth also as trainables
        self.weight = torch.nn.Parameter(self.weight)
        self.exp_bit = torch.nn.Parameter(self.exp_bit)
        self.depth_bit = torch.nn.Parameter(self.depth_bit)
        ...
    
    def size_layer(self):
        """
        given by equation 4 : I*H*W * sum(b(i,l)
        Where O , I , H and W are the output, input, height, and
        width dimensions (so shape) of the weight tensor of layer l respec-
        tively, and b(i,l) is the bit depth of output channel i of layer l.
        """
        prods = torch.as_tensor(self.weight.shape[1:])
        size = torch.mul_reduce(prods) *  torch.sum(torch.relu(self.depth_bit))
        return size

    def _quantized_weight(self):
        b = torch.relu(self.depth_bit)
        x_upscaled = self.weight/torch.exp2(self.exp_bit)
        half = torch.exp2(b -1)
        x_clipped = torch.clip(x_upscaled,-1*half,half-1)
        x_round = torch.steRound(x_clipped)
        return torch.exp2(self.exp_bit) * x_round

    

    def __call__(self,x):
        # quantize every forward pass
        W = self._quantized_weight()
        # assert self.weight.shape==W.shape
        # valid padding or should we do same.. paper does not say
        return torch.nn.functional.conv2d(x,W,padding=1)
        
        


class QlinearMLP(torch.nn.Module):
    """
    Linear layer .. we will only use this to make mlp head and not attention weights
    tie each column to exp and depth bit. or else (m*n) weight would make (m*n)*3 trainable weights.
    i.e we will try to prune column weights
    """

    def __init__(self,m:int,n:int,b=2,e=-8):
        self.m , self.n = m,n
        super().__init__()
        b = torch.as_tensor(b)
        e = torch.as_tensor(e)
        # note that torch.nn.Linear(12,8).weight has a shape of 8,12
        # this is already a nn.Parameter.. no need to wrap
        self.linear = torch.nn.Linear(m,n)
        # print(self.linear)
        self.depth_bit = torch.ones(1,m) * b
        self.exp_bit = torch.ones(1,m) * e

        self.depth_bit = torch.nn.Parameter(self.depth_bit)
        self.exp_bit = torch.nn.Parameter(self.exp_bit)

    def size_layer(self):
        # we will just return number of columns
        return torch.tensor(self.m)

    def _quantized_weight(self):
        b = torch.relu(self.depth_bit)
        # print(self.linear.weight.shape , self.exp_bit.shape) # should be (m,n) * (1,n)
        x_upscaled = self.linear.weight/torch.exp2(self.exp_bit)
        half = torch.exp2(b -1)
        x_clipped = torch.clip(x_upscaled,-1*half,half-1)
        x_round = torch.steRound(x_clipped)
        return torch.exp2(self.exp_bit) * x_round

    def __call__(self,x):
        # quantize weight every forward pass
        W = self._quantized_weight()
        return torch.nn.functional.linear(x,W,bias=self.linear.bias)
        
        


with torch.no_grad():
    W1 = QlinearMLP(8,12)
    x = torch.randn((32,8))
    out = W1(x)
    assert out.shape == torch.Size((32,12))
    plt.imshow(W1.linear.weight.float(),cmap="Blues")


@torch.no_grad
def inspect_weights(out,layer):
    assert isinstance(layer,Qconv), "Hein? should be Qconv layer"
    out = out.to("cpu")
    kernel = layer._quantized_weight().to("cpu")
    _,in_channels,k,_ = kernel.shape
    if in_channels>1:
        kernel = kernel[:,0,:,:][:,None,:,:]
    
    kernel_plot = einops.rearrange(kernel,"out_ch in_ch k1 k2 ->  (in_ch k1) (out_ch k2)")
    out_plot = einops.rearrange(out,"b c h w -> (b h) (c w)")


    plt.figure(figsize=(15,8))
    plt.imshow(kernel_plot.float(),cmap="gray")
    plt.show()
    plt.figure(figsize=(15,8))
    plt.imshow(out_plot.float(),cmap="Blues")
    plt.show()
    return out_plot,kernel_plot
    


input_img = torch.as_tensor(img[:5]) # 1,28,28
qconv_layer1 = Qconv(1,32)
qconv_layer2 = Qconv(32,48)

out1 = qconv_layer1(input_img)
out2 = qconv_layer2(out1)

inspect_weights(out1,qconv_layer1)
inspect_weights(out2,qconv_layer2)



class QconvModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = Qconv(1, 16,b=2)
        self.conv2 = Qconv(16,16,b=2)
        self.conv3 = Qconv(16,32,b=1.5,e=-7)
        self.conv4 = Qconv(32,32,b=1.3,e=-6)
        
        self.pool = torch.nn.MaxPool2d(2, 2)

        self.bn1 = torch.nn.BatchNorm2d(16)
        self.bn2 = torch.nn.BatchNorm2d(32)

        self.L1 = QlinearMLP(32*7*7, 32)
        self.L2 = QlinearMLP(32, 10)


    def forward(self, x):
        # (2 convolutions -> bn -> pool) * 2 
        conv1_out = torch.gelu(self.conv1(x))
        conv2_out = torch.gelu(self.conv2(conv1_out))
        conv2_out = self.bn1(conv2_out)
        pool1 = self.pool(conv2_out)

        conv3_out = torch.gelu(self.conv3(pool1))
        conv4_out = torch.gelu(self.conv4(conv3_out))
        conv4_out = self.bn2(conv4_out)
        pool2 = self.pool(conv4_out)

        # then linear layers
        flat = torch.flatten(pool2, 1) # flatten except batch
        l1 = torch.gelu(self.L1(flat))
        l2 = torch.gelu(self.L2(l1))
        logits = l2
        return logits


class QTrainer:
    """
    we will use bfloat16 training on h100...
    for my pc we will just do fp32 training
    """

    def __init__(self,dtype=torch.float32):
        self.model = QconvModel()
        assert isinstance(dtype,torch.dtype),"Hein? unrecognized dtype"
        self.dtype = dtype

        # does not work on my machine with less precision...
        # if self.dtype == torch.float32:
        #     # dont need for high precision right ?
        #     torch.set_float32_matmul_precision("medium")

        print(f"Training with {self.dtype=} precision")


        # self.scaler = torch.amp.GradScaler()
        
        self.model = self.model.to(self.dtype)
        self.model = torch.compile(self.model)
        self.model.to("cuda")
        self.track_decay = []
        self.track_activekernels = []
        self.track_loss = []

    
        self.optim = torch.optim.AdamW(
            self.model.parameters(),
            weight_decay=1e-3)
    
        self.gamma = (1/10) # high for drama!! but should be around 0.05 or something.. compression factor
        # we need to calculate total number of parameters at initialization (papaer calls it N)
        # here since everything is trainable

        self.tot_init = sum(p.numel() for group in self.optim.param_groups for p in group['params'] if p.requires_grad)
        self.tot_qparams = torch.sum(torch.tensor([p_weight.numel() for p,p_weight in self.model.named_parameters() if "_bit" in p]))
    
        print(f"Total Parameters {self.tot_init=}")
        print(f"of which compression are :{self.tot_qparams=}")
        print(f"compression factor at init {self.gamma * self._qlayersize()}")
    
        # print(self._qlayersize())
        print(self._activekernelscount())


    def _qlayersize(self):
        size_conv = torch.sum(torch.tensor([layer.size_layer() for layer in self.model.modules() if isinstance(layer,Qconv)]))
        size_lin =  torch.sum(torch.tensor([layer.size_layer() for layer in self.model.modules() if isinstance(layer,QlinearMLP)]))
        # [print("->",layer,layer.size_layer()) for layer in self.model.modules() if isinstance(layer,QlinearMLP)]
        # print(size_lin,size_conv)
        return (size_conv + size_lin)/self.tot_init


    def _activekernelscount(self):
        kernel_counts = dict()
        for name,layer in self.model.named_modules():
            if isinstance(layer,Qconv):
                depths = torch.relu(layer.depth_bit)
                count =torch.sum(torch.where(depths>0,1,0)).item()
                kernel_counts[name] = count
        return kernel_counts
        

    
    # @torch.compile # this will not work if you want to track modules states in dict and such.. dynamo error. compile model instead.
    def train(self,num_epochs=10):
        pbar_epoch = tqdm(range(num_epochs))
        for epoch in pbar_epoch:
            i = 0
            for batch_img, batch_label in train_loader:
                
                batch_img = batch_img.to("cuda").to(self.dtype)
                batch_label = batch_label.to("cuda")

                # bfloat16 with cross entropy needs autocasting 
                with torch.autocast(device_type="cuda",dtype=self.dtype):
                    out = self.model(batch_img)
                    bit_decay = self._qlayersize()
                    loss = torch.nn.functional.cross_entropy(input=out,target=batch_label) + self.gamma * bit_decay

                self.optim.zero_grad() # commented (bugged) out to show quick grad accum which drops conv filters
                loss.backward()
                self.optim.step()
                i = i +1
                if i %50 == 0:
                    activekernels = self._activekernelscount()
                    pbar_epoch.set_postfix(
                        loss=loss.item(),
                        decay=self.gamma*bit_decay.item(),
                        activekernels = activekernels.values(),
                    )
                    self.track_activekernels.append(activekernels.values())
                    self.track_decay.append(bit_decay.item())
                    self.track_loss.append(loss.item())

        return self.model
    



qmodel = QTrainer(dtype=torch.float32)


watch(qmodel.model)
qmodel.train(50)
log_model(experiment, model=qmodel.model, model_name="CompressingQconv")


with torch.no_grad():
    out_model = qmodel.model
    logits =out_model(img.to("cuda"))
    response = torch.argmax(logits,dim=-1)
    # print(logits)
    print("score:",torch.sum(response==labels.to("cuda")))
    print("of:",labels.shape)
    print("=======Accuracy========")


with torch.no_grad():
    out_model = qmodel.model
    out1 = out_model.conv1(input_img.to("cuda"))
    out2 = out_model.conv2(out1)
    out2 = out_model.bn1(out2)
    
    pool1 = out_model.pool(out2)
    out3 = out_model.conv3(pool1)
    out4 = out_model.conv4(out3)
    
    out,w = inspect_weights(out4,out_model.conv4)
    print(w.shape)
    experiment.log_image(out)
    display(sp.Matrix(w))


experiment.end()



